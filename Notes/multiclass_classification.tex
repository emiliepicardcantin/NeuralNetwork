When the labels can be more than 0 or 1, we need a different activation function than the typical sigmoid function.
Usually, the softmax function is used as the last layer of the neural network.

Let $C$ be the number of possible classes and $m$ be the number of examples in the dataset.
The labels $Y\in\{0,1\}^{C\times m}$ are such that $Y_k^{(i)} = 1$ if example $i$ is labeled with class $k$.

\begin{align}
	Z^{[j]} &= W^{[j]} A^{[j-1]} + b^{[j]} \\
	A^{[j]} &= \frac{e^{Z^{[j]} }}{\sum_{k=1}^{C} e^{Z^{[j]}_k }}
\end{align}

The loss function also requires some adaptation.

\begin{align}
	\mathcal{L}(\hat{y}, y) &= - \sum_{j=1}^{C} y_j \log(\hat{y}_j) \\
	J(\hat{Y}, Y) &= \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(\hat{Y}^{(i)}, Y^{(i)})
\end{align}